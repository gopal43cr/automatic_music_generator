# Automatic Music Generator

This project uses a Transformer neural network to generate music from MIDI files. The model is trained on a dataset of MIDI files and learns to predict the next note given a sequence of previous notes. The generated music is then saved as a MIDI file.

## Installation
- Python 3.6 or higher
- `torch`
- `numpy`
- `sklearn`
- `music21`
- `tqdm`

### Steps

1. Clone the repository:
    ```bash
    git clone https://github.com/gopal43cr/automatic_music_generator.git
    cd automatic_music_generator
    ```

2. Install the required libraries:
    ```bash
    pip install torch numpy scikit-learn music21 tqdm
    ```

3. Place your MIDI files in the `data` folder.

4. Run the scripts in the following order
   1. ```bash python load_data.py ```
   2. ```bash python preprocess.py ```
   3. ```bash python model.py ```
   4. ```bash python train.py ```
   5. ```bash python generate.py ```
   6. ```bash python creatMIDI.py ```
      
5. After running the train.py scripts, in the 'models' folder one file will be saved as 'transformer_model.pth'
   
4. After running the creatMIDI.py script, the generated MIDI files will be saved in the `output` folder.

## Music Generation

Music is generated by predicting the next note in a sequence. The model uses a seed sequence from the test dataset and generates a sequence of 200 notes. The generated notes are converted to a MIDI file and saved in the `output` folder. Now you can convert the MIDI files to mp3 files and enjoy the new music :))

## Future Scope

1. I don't have a GPU that's why I only did the training for one epoch, you can do any arbitrary times you want. You can change the hyperparameters to get a low validation loss. I have run 
   this model in Google Colab(used GPU) and it had a validation loss of 1.8
2. I have tokenized the notes with some numbers and then I predicted the next number provided I have a sequence of 50 numbers. Basically, my neural network is trying to find the pattern. So 
   you can tokenize the raw data (notes and chords) in such a way that it can be represented in a 'n' dimensional space in the embedding table and then you can predict the next token with 
   the help of 'transformer'.
3. I am only extracting the instrument part of the music. You can extract other parts as well and represent them in a high-dimensional space

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
